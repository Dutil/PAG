'''
====Attributions====
Copyright (c) 2011--2014, Université de Montréal
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
 
 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
 
 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
 
 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
 
 THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

'''

from commons import EPS
import theano.tensor as TT
from utils import as_floatX
from theano.tensor.extra_ops import cumsum


def sparsity_penalty(h, sparsity_level=0.05, sparse_reg=1e-4):

    if h.ndim == 2:
        sparsity_level = TT.extra_ops.repeat(sparsity_level, h.shape[1])
    else:
        sparsity_level = TT.extra_ops.repeat(sparsity_level, h.shape[0])
    sparsity_penalty = 0

    avg_act = h.mean(axis=0)
    kl_div = kl_divergence(sparsity_level, avg_act)
    sparsity_penalty = sparse_reg * kl_div.sum()

    # Implement KL divergence here.
    return sparsity_penalty


def kl_simple(Y, Y_hat, cost_mask=None):
    """
    Warning: This function expects a sigmoid nonlinearity in the
    output layer. Returns a batch (vector) of mean across units of
    KL divergence for each example,
    KL(P || Q) where P is defined by Y and Q is defined by Y_hat:
    p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
    For binary p, some terms drop out:
    - p log q - (1-p) log (1-q)
    - p log sigmoid(z) - (1-p) log sigmoid(-z)
    p softplus(-z) + (1-p) softplus(z)
    Parameters
    ----------
    Y : Variable
        targets for the sigmoid outputs. Currently Y must be purely binary.
        If it's not, you'll still get the right gradient, but the
        value in the monitoring channel will be wrong.
    Y_hat : Variable
        predictions made by the sigmoid layer. Y_hat must be generated by
        fprop, i.e., it must be a symbolic sigmoid.
    -------
    ave : Variable
        average kl divergence between Y and Y_hat.
    """

    assert hasattr(Y_hat, 'owner')

    owner = Y_hat.owner
    assert owner is not None
    op = owner.op

    if not hasattr(op, 'scalar_op'):
        raise ValueError("Expected Y_hat to be generated by an Elemwise "
                         "op, got "+str(op)+" of type "+str(type(op)))

    assert isinstance(op.scalar_op, TT.nnet.sigm.ScalarSigmoid)

    z, = owner.inputs

    z = z.reshape(Y.shape)
    term_1 = Y * TT.nnet.softplus(-z)
    term_2 = (1 - Y) * TT.nnet.softplus(z)

    total = term_1 + term_2
    if cost_mask is not None:
        total = cost_mask * total

    ave = total.sum() / (total.shape[1] * (total.shape[2] - 2))
    #ave = ave.mean()
    return ave


def kl(Y, Y_hat, cost_mask=None,
       batch_vec=True,
       cost_matrix=False,
       sum_tru_time=False,
       normalize_by_outsize=True):

    """
    Warning: This function expects a sigmoid nonlinearity in the
    output layer. Returns a batch (vector) of mean across units of
    KL divergence for each example,
    KL(P || Q) where P is defined by Y and Q is defined by Y_hat:
    p log p - p log q + (1-p) log (1-p) - (1-p) log (1-q)
    For binary p, some terms drop out:
    - p log q - (1-p) log (1-q)
    - p log sigmoid(z) - (1-p) log sigmoid(-z)
    p softplus(-z) + (1-p) softplus(z)
    Parameters
    ----------
    Y : Variable
        targets for the sigmoid outputs. Currently Y must be purely binary.
        If it's not, you'll still get the right gradient, but the
        value in the monitoring channel will be wrong.
    Y_hat : Variable
        predictions made by the sigmoid layer. Y_hat must be generated by
        fprop, i.e., it must be a symbolic sigmoid.
    -------
    ave : Variable
        average kl divergence between Y and Y_hat.
    """

    assert hasattr(Y_hat, 'owner')

    owner = Y_hat.owner
    assert owner is not None
    op = owner.op

    if not hasattr(op, 'scalar_op'):
        raise ValueError("Expected Y_hat to be generated by an Elemwise "
                         "op, got "+str(op)+" of type "+str(type(op)))

    assert isinstance(op.scalar_op, TT.nnet.sigm.ScalarSigmoid)

    z, = owner.inputs
    z = z.reshape(Y.shape)
    term_1 = Y * TT.nnet.softplus(-z)
    term_2 = (1 - Y) * TT.nnet.softplus(z)

    total = term_1 + term_2

    if cost_mask is not None:
        if cost_mask.ndim != total.ndim:
            cost_mask = cost_mask.dimshuffle(0, 1, 'x')

        total = cost_mask * total
    if not sum_tru_time:
        if normalize_by_outsize:
            if cost_matrix:
                ave = total.sum(-1) / TT.cast((total.shape[2] - 2), "float32")
            else:
                if batch_vec:
                    ave = total.sum(0).sum(1) / TT.cast((total.shape[2] - 2), "float32")
                else:
                    ave = total.sum() / (total.shape[1] * (total.shape[2] - 2))
        else:
            if cost_matrix:
                ave = total.sum(-1)
            else:
                if batch_vec:
                    ave = total.sum(0).sum(1)
                else:
                    ave = total.sum() / TT.cast(total.shape[1], "float32")
    else:
        assert not cost_matrix
        if normalize_by_outsize:
            if batch_vec:
                ave = cumsum(total.sum(-1) / TT.cast((total.shape[2] - 2), "float32"),
                        axis=0)[::-1]
            else:
                ave = cumsum(total.sum((1, 2)) / (total.shape[1] * (total.shape[2] - 2)),
                        axis=0)[::-1]
        else:
            if batch_vec:
                ave = cumsum(total.sum(-1), axis=0)[::-1]
            else:
                ave = cumsum(total.sum((1, 2)) / TT.cast(total.shape[1], "float32"), axis=0)[::-1]

    return ave


def _grab_probs(class_probs, target, use_fast_ver=False):
    if class_probs.ndim == 3:
        class_probs = class_probs.reshape((-1, class_probs.shape[-1]))

    shape0 = class_probs.shape[0]
    shape1 = class_probs.shape[1]

    p = None
    if target.ndim == 2 and use_fast_ver:
        target = target.flatten()
        cp = class_probs.reshape((target.shape[0], -1))
        p = TT.diag(cp.T[target])
    else:
        if target.ndim > 1:
            target = target.flatten()
        assert target.ndim == 1, 'make sure target is a vector of ints'
        assert 'int' in target.dtype
        pos = TT.arange(shape0)*shape1
        new_targ = target + pos
        #p = class_probs.flatten().dimshuffle(0, 'x')[new_targ, :]
        p = class_probs.reshape((shape0*shape1, 1))[new_targ].reshape((shape0,))
    return p


def nll_simple(Y, Y_hat, cost_mask=None):

    probs = Y_hat
    pred = TT.argmax(probs, axis=1).reshape(Y.shape)
    errors = TT.neq(pred, Y)

    LL = TT.log(_grab_probs(probs, Y) + 1e-8).reshape(Y.shape)

    if cost_mask is not None:
        total = cost_mask * LL
        errors = cost_mask * errors
        ncosts = TT.sum(cost_mask)
        mean_errors = TT.sum(errors) / (ncosts)
        ave = -TT.sum(total) / Y.shape[1]
    else:
        mean_errors = TT.mean(errors)
        ave = -TT.sum(LL) / Y.shape[0]
    return ave, mean_errors


def nll(Y, Y_hat, cost_mask=None, batch_vec=True, cost_matrix=False, use_fast_ver=False):
    probs = Y_hat
    pred = TT.argmax(probs, axis=-1).reshape(Y.shape)
    errors = TT.neq(pred, Y)
    LL = TT.log(_grab_probs(probs, Y, use_fast_ver=use_fast_ver) + 1e-8).reshape(Y.shape)

    if cost_mask is not None:
        total = cost_mask * LL
        errors = cost_mask * errors
        ncosts = TT.sum(cost_mask)
        mean_errors = TT.sum(errors) / (ncosts)
        if cost_matrix:
            ave = -total
        else:
            if batch_vec:
                ave = -total.sum(0)
            else:
                ave = -TT.sum(total) / Y.shape[1]
    else:
        mean_errors = TT.mean(errors)
        if cost_matrix:
            ave = -LL
        else:
            if batch_vec and LL.ndim == 2:
               ave = -LL.sum(0)
            elif batch_vec and LL.ndim == 1:
               ave = -LL
            else:
               if Y.ndim == 2:
                   ave = -TT.sum(LL) / Y.shape[1]
               else:
                   ave = -LL.mean()

    return ave, mean_errors


def nll_hints(Y, Y_hat, npatches=64, cost_mask=None, cost_matrix=False, use_mask=False):
    if use_mask:
        mask = Y > 0
        mask = TT.set_subtensor(mask[:, -1], 1)
        mask = TT.cast(mask, "float32")

    if Y_hat.ndim != 3:
        probs = Y_hat.reshape((Y.shape[0], npatches, -1))
    else:
        probs = Y_hat

    LL = TT.log(_grab_probs(probs, Y) + 1e-8).reshape((Y.shape[0], Y.shape[1], -1))

    if use_mask:
        LL = mask.dimshuffle(0, 1, 'x') * LL

    cost = -LL
    pred = TT.argmax(probs, axis=-1)
    errors = TT.neq(pred, Y)

    mean_errors = errors.mean()
    mean_cost = cost.mean(0).sum()
    return mean_cost, mean_errors


def huber_loss(y_hat, target, delta=1, center=0, std=1):

    l1_diff = abs((target - center - y_hat) / std)
    huber_loss = TT.switch(TT.ge(l1_diff, delta),
                           (2*l1_diff - 1) * delta,
                           l1_diff**2)
    return huber_loss
